{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Victoria Espinola\n",
    "## BNQ164\n",
    "# Lab 09\n",
    "### IS6713\n",
    "\n",
    "Instructions:\n",
    "1. Add the lines of code required to obtain the requested output.\n",
    "\n",
    "2. The code should be inserted where requested   \n",
    "   \n",
    "3. Do not modify other parts of this notebook.  \n",
    "  \n",
    "4. Remember to write in your name and abc123 in the first cell of this notebook.\n",
    "  \n",
    "5. Upload the notebook on Blackboard before the deadline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.1\n",
    "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").  \n",
    "The data I am providing is a reformatted version of the Kaggle one. For example, \"neutral\" Tweets are not included for simplicity. The dataset includes three fields: **airline**, **full_text**, and **gold_label**.  \n",
    "Tasks:\n",
    "- Load the **w9_airlinetwitter.jsonl** dataset\n",
    "- Vectorize using the TFIDF approach\n",
    "- Split the dataset in train (80%) and test (20%) sets\n",
    "- Train the RandomForest Classifier over the train set\n",
    "- Predict the labels of the test set using the trained model\n",
    "- Calculate and print:\n",
    "  - Classification report\n",
    "  - Confusion matrix\n",
    "  - Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1: load the dataset\n",
    "## Don't modify this code\n",
    "dataset = []\n",
    "with open('w9_airlinetwitter.jsonl') as infile:\n",
    "    for line in infile:\n",
    "        dataset.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Correctly Loaded\n"
     ]
    }
   ],
   "source": [
    "assert(len(dataset)==11541)\n",
    "print(\"Dataset Correctly Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airline': 'Virgin America',\n",
       " 'full_text': \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
       " 'gold_label': 'positive'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the format (don't modify this cell)\n",
    "## It should be something like:\n",
    "\"\"\"{'airline': 'Virgin America',\n",
    " 'full_text': \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
    " 'gold_label': 'positive'}\n",
    " \"\"\"\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2 - Vectorize the dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "vectorizer=TfidfVectorizer(max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "processed_features_vec = vectorizer.fit_transform([dictionary['full_text'] for dictionary in dataset]).toarray()\n",
    "gold_labels=np.array([dictionary['gold_label'] for dictionary in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3 - Split in train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "processed_features_train, processed_features_test, gold_labels_train, gold_labels_test = train_test_split(processed_features_vec, gold_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 4: train the classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(processed_features_train, gold_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Predict the labels\n",
    "predictions = text_classifier.predict(processed_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n",
      "[[1795   65]\n",
      " [ 175  274]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.97      0.94      1860\n",
      "    positive       0.81      0.61      0.70       449\n",
      "\n",
      "    accuracy                           0.90      2309\n",
      "   macro avg       0.86      0.79      0.82      2309\n",
      "weighted avg       0.89      0.90      0.89      2309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Step 6: Assess the performance (Accuracy, Confusion Matrix, Classification Report)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(gold_labels_test, predictions).ravel()\n",
    "print(\"Accuracy: {0:.2f}\".format(accuracy_score(gold_labels_test, predictions)))\n",
    "print(confusion_matrix(gold_labels_test, predictions))\n",
    "print(classification_report(gold_labels_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9.2 \n",
    "### IRIS\n",
    "Do the following:\n",
    "1. Load the **iris.csv** dataset into a numpy array. The first 4 columns are the  features/attributes. The last column is the class. Simply load the class as a list of strings. You can use the CVS method to load the features into a list of lists (X) and to load the gold label as a list (y). Then, convert X and y into a numpy array (hint: use np.array() for the conversion).\n",
    "2. Use train_test_split() to split X and y into train and test. (use 0.2 for the test_size)\n",
    "3. Train an SVM classifier on the train split and evaluate its accuracy on the test split. Print the accuracy.\n",
    "4. Apply PCA to the array X, reducing the number of features to 2. Save the new set of features in a different array (e.g. newX)\n",
    "4. Split the new dataset (newX) and y in train and test\n",
    "5. Train an SVM classifier on the train split and evaluate the accuracy on the test. Print the new accuracy.\n",
    "6. Compare the results from 3) and 5). Which one is better?\n",
    "  \n",
    "Note that this is a toy dataset, so all scores will be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from csv import reader\n",
    "import numpy as np\n",
    "X = [] # Will be a list of lists\n",
    "y = [] # will be a list\n",
    "\n",
    "## Write here your code\n",
    "## read the file\n",
    "fhand=open('iris.csv', 'r')\n",
    "for line in reader(fhand):\n",
    "    X.append(line[0:3]),y.append(line[-1])\n",
    "X,y=np.array(X),np.array(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split into train and test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train an SVM Classifier, test and print the accuracy\n",
    "clf = SVC(C=1.0) \n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(\"accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset (150, 3) Data after PCA (120, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Run a PCA to select only 2 components and create a new dataset\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_scale=ss.transform(X_train)\n",
    "pca = PCA(n_components =2)\n",
    "pca.fit(X_scale)\n",
    "X_train_components=pca.transform(X_scale)\n",
    "print(\"Original dataset\", X.shape,\"Data after PCA\", new_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_scaled=ss.transform(X_test)\n",
    "X_test_components=pca.transform(test_scaled)\n",
    "\n",
    "## Split the new dataset, train a SVM classifier, test and print the accuracy\n",
    "clf = SVC(C=1.0) \n",
    "clf.fit(X_train_components,y_train)\n",
    "y_pred=clf.predict(X_test_components)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "print(\"accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment about the differences. Is the difference big or small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Comment here\n",
    "The model was more accurate before applying PCA. The difference was about 7%. This difference could be explained by the\n",
    "original dataset only containing 3 features. \n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
